


@article{1,
author = {Ben Taieb, Souhaib and Bontempi, Gianluca and Atiya, Amir and Sorjamaa, Antti},
year = {2011},
month = {08},
pages = {},
title = {A review and comparison of strategies for multi-step ahead time series
forecasting based on the NN5 forecasting competition},
volume = {39},
journal = {Expert Systems with Applications},
doi = {10.1016/j.eswa.2012.01.039}
}

@inproceedings{2,
author = {Sorjamaa, Antti and Lendasse, Amaury},
year = {2006},
month = {01},
pages = {143-148},
title = {Time Series Prediction using DirRec Strategy},
volume = {6}
}
@TECHREPORT{3,
title = {Recursive and direct multi-step forecasting: the best of both worlds},
author = {Ben Taieb, Souhaib and Hyndman, Rob},
year = {2012},
institution = {Monash University, Department of Econometrics and Business Statistics},
type = {Monash Econometrics and Business Statistics Working Papers},
number = {19/12}
}

@article{4,
	doi = {10.1057/jors.2014.103},
  
	url = {https://doi.org/10.1057\%2Fjors.2014.103},
  
	year = 2015,
	month = {aug},
  
	publisher = {Informa {UK} Limited},
  
	volume = {66},
  
	number = {8},
  
	pages = {1352--1362},
  
	author = {Chris Tofallis},
  
	title = {A better measure of relative prediction accuracy for model selection and model estimation},
  
	journal = {Journal of the Operational Research Society}
}

@article{5,
author = {Hyndman, Rob},
year = {2006},
month = {01},
pages = {43-46},
title = {Another Look at Forecast Accuracy Metrics for Intermittent Demand},
volume = {4},
journal = {Foresight: The International Journal of Applied Forecasting}
}

@article{6,
title = {A new metric of absolute percentage error for intermittent demand forecasts},
journal = {International Journal of Forecasting},
volume = {32},
number = {3},
pages = {669-679},
year = {2016},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2015.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169207016000121},
author = {Sungil Kim and Heeyoung Kim},
keywords = {Accuracy measure, Forecast evaluation, Intermittent demand, MAPE},
abstract = {The mean absolute percentage error (MAPE) is one of the most widely used measures of forecast accuracy, due to its advantages of scale-independency and interpretability. However, MAPE has the significant disadvantage that it produces infinite or undefined values for zero or close-to-zero actual values. In order to address this issue in MAPE, we propose a new measure of forecast accuracy called the mean arctangent absolute percentage error (MAAPE). MAAPE has been developed through looking at MAPE from a different angle. In essence, MAAPE is a slope as an angle, while MAPE is a slope as a ratio, considering a triangle with adjacent and opposite sides that are equal to an actual value and the difference between the actual and forecast values, respectively. MAAPE inherently preserves the philosophy of MAPE, overcoming the problem of division by zero by using bounded influences for outliers in a fundamental manner through considering the ratio as an angle instead of a slope. The theoretical properties of MAAPE are investigated, and the practical advantages are demonstrated using both simulated and real-life data.}
}

@article{7,
	doi = {10.1016/j.neucom.2015.12.114},
  
	url = {https://doi.org/10.1016%2Fj.neucom.2015.12.114},
  
	year = 2016,
	month = {jun},
  
	publisher = {Elsevier {BV}
},
  
	volume = {192},
  
	pages = {38--48},
  
	author = {Arnaud de Myttenaere and Boris Golden and B{\'{e}}n{\'{e}}dicte Le Grand and Fabrice Rossi},
  
	title = {Mean Absolute Percentage Error for regression models},
  
	journal = {Neurocomputing}
}

@article{8,
title = {Accuracy measures: theoretical and practical concerns},
journal = {International Journal of Forecasting},
volume = {9},
number = {4},
pages = {527-529},
year = {1993},
issn = {0169-2070},
doi = {https://doi.org/10.1016/0169-2070(93)90079-3},
url = {https://www.sciencedirect.com/science/article/pii/0169207093900793},
author = {Spyros Makridakis}
}

@article{9,
author = {Chen, Zhuo and Yang, Yuhong},
year = {2004},
month = {04},
pages = {},
title = {Assessing forecast accuracy measures}
}

@misc{10,
  title = {Errors on percentage errors},
  howpublished = {\url{https://robjhyndman.com/hyndsight/smape/}},
  note = {Accessed: 2022-09-04}
}

@ARTICLE{11,
title = {Long-range forecasting: From crystal ball to computer: J. Scott Armstrong, 2nd ed. (Wiley, New York, 1985) [UK pound]22.95 (paper), pp. 689},
author = {Schnaars, Steven P.},
year = {1986},
journal = {International Journal of Forecasting},
volume = {2},
number = {3},
pages = {387-390},
url = {https://EconPapers.repec.org/RePEc:eee:intfor:v:2:y:1986:i:3:p:387-390}
}

@article{12,
  title={The coefficient of determination R-squared is more informative than SMAPE, MAE, MAPE, MSE and RMSE in regression analysis evaluation},
  author={Chicco, Davide and Warrens, Matthijs J and Jurman, Giuseppe},
  journal={PeerJ Computer Science},
  volume={7},
  pages={e623},
  year={2021},
  publisher={PeerJ Inc.}
}

@article{13,
title = {Another look at measures of forecast accuracy},
journal = {International Journal of Forecasting},
volume = {22},
number = {4},
pages = {679-688},
year = {2006},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2006.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169207006000239},
author = {Rob J. Hyndman and Anne B. Koehler},
keywords = {Forecast accuracy, Forecast evaluation, Forecast error measures, M-competition, Mean absolute scaled error},
abstract = {We discuss and compare measures of accuracy of univariate time series forecasts. The methods used in the M-competition as well as the M3-competition, and many of the measures recommended by previous authors on this topic, are found to be degenerate in commonly occurring situations. Instead, we propose that the mean absolute scaled error become the standard measure for comparing forecast accuracy across multiple time series.}
}

@inbook{14,
author = {Bontempi, Gianluca and Ben Taieb, Souhaib and Le Borgne, Yann-Aël},
year = {2013},
month = {01},
pages = {},
title = {Machine Learning Strategies for Time Series Forecasting},
volume = {138},
isbn = {978-3-642-36317-7},
journal = {Lecture Notes in Business Information Processing},
doi = {10.1007/978-3-642-36318-4_3}
}

@misc{15,
  doi = {10.48550/ARXIV.1108.3259},
  
  url = {https://arxiv.org/abs/1108.3259},
  
  author = {Taieb, Souhaib Ben and Bontempi, Gianluca and Atiya, Amir and Sorjamaa, Antti},
  
  keywords = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Applications (stat.AP), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition},
  
  publisher = {arXiv},
  
  year = {2011},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{16,
  title={Multilayer feedforward networks are universal approximators},
  author={Kurt Hornik and Maxwell B. Stinchcombe and Halbert L. White},
  journal={Neural Networks},
  year={1989},
  volume={2},
  pages={359-366}
}

@Book{17,
  Title                    = {Deep Learning},
  Author                   = {Ian J. Goodfellow and Yoshua Bengio and Aaron Courville},
  Publisher                = {MIT Press},
  Year                     = {2016},

  Address                  = {Cambridge, MA, USA},
  Note                     = {\url{http://www.deeplearningbook.org}}
}

@article{18,
author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
year = {1997},
month = {12},
pages = {1735-80},
title = {Long Short-term Memory},
volume = {9},
journal = {Neural computation},
doi = {10.1162/neco.1997.9.8.1735}
}

@ARTICLE{19,
  
AUTHOR={Natekin, Alexey and Knoll, Alois},   
	 
TITLE={Gradient boosting machines, a tutorial},      
	
JOURNAL={Frontiers in Neurorobotics},      
	
VOLUME={7},      
	
YEAR={2013},      
	  
URL={https://www.frontiersin.org/article/10.3389/fnbot.2013.00021},       
	
DOI={10.3389/fnbot.2013.00021},      
	
ISSN={1662-5218},   
   
ABSTRACT={Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods with a strong focus on machine learning aspects of modeling. A theoretical information is complemented with descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. Three practical examples of gradient boosting applications are presented and comprehensively analyzed.}
}

@article{20,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@Inbook{21,
author="Rojas, Ra{\'u}l",
title="The Backpropagation Algorithm",
bookTitle="Neural Networks: A Systematic Introduction",
year="1996",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="149--182",
abstract="We saw in the last chapter that multilayered networks are capable of computing a wider range of Boolean functions than networks with a single layer of computing units. However the computational effort needed for finding the correct combination of weights increases substantially when more parameters and more complicated topologies are considered. In this chapter we discuss a popular learning method capable of handling such large learning problems---the backpropagation algorithm. This numerical method was used by different research communities in different contexts, was discovered and rediscovered, until in 1985 it found its way into connectionist AI mainly through the work of the PDP group [382]. It has been one of the most studied and used algorithms for neural networks learning ever since.",
isbn="978-3-642-61068-4",
doi="10.1007/978-3-642-61068-4_7",
url="https://doi.org/10.1007/978-3-642-61068-4_7"
}

@ARTICLE{22,  author={Werbos, P.J.},  journal={Proceedings of the IEEE},   title={Backpropagation through time: what it does and how to do it},   year={1990},  volume={78},  number={10},  pages={1550-1560},  doi={10.1109/5.58337}}

@incollection{23,
  title={Decision tree learning},
  author={Suthaharan, Shan},
  booktitle={Machine Learning Models and Algorithms for Big Data Classification},
  pages={237--269},
  year={2016},
  publisher={Springer}
}

@article{24,
title = {Stacked generalization},
journal = {Neural Networks},
volume = {5},
number = {2},
pages = {241-259},
year = {1992},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(05)80023-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608005800231},
author = {David H. Wolpert},
keywords = {Generalization and induction, Combining generalizers, Learning set preprocessing, cross-validation, Error estimation and correction},
abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.}
}


@Article{25,
  author={Sean J. Taylor and Benjamin Letham},
  title={{Forecasting at Scale}},
  journal={The American Statistician},
  year=2018,
  volume={72},
  number={1},
  pages={37-45},
  month={January},
  keywords={},
  doi={10.1080/00031305.2017.138},
  abstract={ Forecasting is a common data science task that helps organizations with capacity planning, goal setting, and anomaly detection. Despite its importance, there are serious challenges associated with producing reliable and high-quality forecasts—especially when there are a variety of time series and analysts with expertise in time series modeling are relatively rare. To address these challenges, we describe a practical approach to forecasting “at scale” that combines configurable models with analyst-in-the-loop performance analysis. We propose a modular regression model with interpretable parameters that can be intuitively adjusted by analysts with domain knowledge about the time series. We describe performance analyses to compare and evaluate forecasting procedures, and automatically flag forecasts for manual review and adjustment. Tools that help analysts to use their expertise most effectively enable reliable, practical forecasting of business time series.},
  url={https://ideas.repec.org/a/taf/amstat/v72y2018i1p37-45.html}
}

@article{26,
title = "Forecasting time series with complex seasonal patterns using exponential smoothing",
abstract = "An innovations state space modeling framework is introduced for forecasting complex seasonal time series such as those with multiple seasonal periods, high-frequency seasonality, non-integer seasonality, and dual-calendar effects. The new framework incorporates Box-Cox transformations, Fourier representations with time varying coefficients, and ARMA error correction. Likelihood evaluation and analytical expressions for point forecasts and interval predictions under the assumption of Gaussian errors are derived, leading to a simple, comprehensive approach to forecasting complex seasonal time series. A key feature of the framework is that it relies on a new method that greatly reduces the computational burden in the maximum likelihood estimation. The modeling framework is useful for a broad range of applications, its versatility being illustrated in three empirical studies. In addition, the proposed trigonometric formulation is presented as a means of decomposing complex seasonal time series, and it is shown that this decomposition leads to the identification and extraction of seasonal components which are otherwise not apparent in the time series plot itself.",
author = "{De Livera}, Alysha and Robin Hyndman and Ralph Snyder",
year = "2011",
doi = "10.1198/jasa.2011.tm09771",
language = "English",
volume = "106",
pages = "1513 -- 1527",
journal = "Journal of the American Statistical Association",
issn = "0162-1459",
publisher = "Taylor & Francis",
number = "496",
}

@article{27,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/2347162},
 abstract = {The Holt-Winters forecasting procedure is a simple widely used projection method which can cope with trend and seasonal variation. However, empirical studies have tended to show that the method is not as accurate on average as the more complicated Box-Jenkins procedure. This paper points out that these empirical studies have used the automatic version of the method, whereas a non-automatic version is also possible in which subjective judgement is employed, for example, to choose the correct model for seasonality. The paper re-analyses seven series from the Newbold-Granger study for which Box-Jenkins forecasts were reported to be much superior to the (automatic) Holt-Winters forecasts. The series do not appear to have any common properties, but it is shown that the automatic Holt-Winters forecasts can often be improved by subjective modifications. It is argued that a fairer comparison would be that between Box-Jenkins and a non-automatic version of Holt-Winters. Some general recommendations are made concerning the choice of a univariate forecasting procedure. The paper also makes suggestions regarding the implementation of the Holt-Winters procedure, including a choice of starting values.},
 author = {C. Chatfield},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {3},
 pages = {264--279},
 publisher = {[Wiley, Royal Statistical Society]},
 title = {The Holt-Winters Forecasting Procedure},
 urldate = {2022-04-21},
 volume = {27},
 year = {1978}
}
@article{28,
title = {Forecasting seasonals and trends by exponentially weighted moving averages},
journal = {International Journal of Forecasting},
volume = {20},
number = {1},
pages = {5-10},
year = {2004},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2003.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0169207003001134},
author = {Charles C. Holt},
keywords = {Exponential smoothing, Forecasting, Local seasonals, Local trends},
abstract = {The paper provides a systematic development of the forecasting expressions for exponential weighted moving averages. Methods for series with no trend, or additive or multiplicative trend are examined. Similarly, the methods cover non-seasonal, and seasonal series with additive or multiplicative error structures. The paper is a reprinted version of the 1957 report to the Office of Naval Research (ONR 52) and is being published here to provide greater accessibility.}
}
@article{29,
 ISSN = {00251909, 15265501},
 URL = {http://www.jstor.org/stable/2627346},
 abstract = {The growing use of computers for mechanized inventory control and production planning has brought with it the need for explicit forecasts of sales and usage for individual products and materials. These forecasts must be made on a routine basis for thousands of products, so that they must be made quickly, and, both in terms of computing time and information storage, cheaply; they should be responsive to changing conditions. The paper presents a method of forecasting sales which has these desirable characteristics, and which in terms of ability to forecast compares favorably with other, more traditional methods. Several models of the exponential forecasting system are presented, along with several examples of application.},
 author = {Peter R. Winters},
 journal = {Management Science},
 number = {3},
 pages = {324--342},
 publisher = {INFORMS},
 title = {Forecasting Sales by Exponentially Weighted Moving Averages},
 urldate = {2022-04-21},
 volume = {6},
 year = {1960}
}

@article{30,
title = {Methodology for long-term prediction of time series},
journal = {Neurocomputing},
volume = {70},
number = {16},
pages = {2861-2869},
year = {2007},
note = {Neural Network Applications in Electrical Engineering Selected papers from the 3rd International Work-Conference on Artificial Neural Networks (IWANN 2005)},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2006.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0925231207001610},
author = {Antti Sorjamaa and Jin Hao and Nima Reyhani and Yongnan Ji and Amaury Lendasse},
keywords = {Time series prediction, Input selection, -Nearest neighbors, Mutual information, Nonparametric noise estimation, Recursive prediction, Direct prediction, Least squares support vector machines},
abstract = {In this paper, a global methodology for the long-term prediction of time series is proposed. This methodology combines direct prediction strategy and sophisticated input selection criteria: k-nearest neighbors approximation method (k-NN), mutual information (MI) and nonparametric noise estimation (NNE). A global input selection strategy that combines forward selection, backward elimination (or pruning) and forward–backward selection is introduced. This methodology is used to optimize the three input selection criteria (k-NN, MI and NNE). The methodology is successfully applied to a real life benchmark: the Poland Electricity Load dataset.}}

@inproceedings{31,
author = {Sorjamaa, Antti and Lendasse, Amaury},
year = {2006},
month = {01},
pages = {143-148},
title = {Time Series Prediction using DirRec Strategy},
volume = {6}
}

@INPROCEEDINGS{32,
  author={An, Nguyen Hoang and Anh, Duong Tuan},
  booktitle={2015 International Conference on Advanced Computing and Applications (ACOMP)}, 
  title={Comparison of Strategies for Multi-step-Ahead Prediction of Time Series Using Neural Network}, 
  year={2015},
  volume={},
  number={},
  pages={142-149},
  doi={10.1109/ACOMP.2015.24}}
  
@article{33,
title = {Beyond one-step-ahead forecasting: Evaluation of alternative multi-step-ahead forecasting models for crude oil prices},
journal = {Energy Economics},
volume = {40},
pages = {405-415},
year = {2013},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2013.07.028},
url = {https://www.sciencedirect.com/science/article/pii/S0140988313001746},
author = {Tao Xiong and Yukun Bao and Zhongyi Hu},
keywords = {Crude oil price forecasting, Multi-step-ahead forecasting, EMD-based modeling framework, End effect, Prediction strategy},
abstract = {An accurate prediction of crude oil prices over long future horizons is challenging and of great interest to governments, enterprises, and investors. This paper proposes a revised hybrid model built upon empirical mode decomposition (EMD) based on the feed-forward neural network (FNN) modeling framework incorporating the slope-based method (SBM), which is capable of capturing the complex dynamic of crude oil prices. Three commonly used multi-step-ahead prediction strategies proposed in the literature, including iterated strategy, direct strategy, and MIMO (multiple-input multiple-output) strategy, are examined and compared, and practical considerations for the selection of a prediction strategy for multi-step-ahead forecasting relating to crude oil prices are identified. The weekly data from the WTI (West Texas Intermediate) crude oil spot price are used to compare the performance of the alternative models under the EMD–SBM–FNN modeling framework with selected counterparts. The quantitative and comprehensive assessments are performed on the basis of prediction accuracy and computational cost. The results obtained in this study indicate that the proposed EMD–SBM–FNN model using the MIMO strategy is the best in terms of prediction accuracy with accredited computational load.}
}

@inbook{34,
author = {Kline, Douglas},
year = {2004},
month = {01},
pages = {226-250},
title = {Methods for Multi-Step Time Series Forecasting with Neural Networks},
isbn = {9781591401773},
journal = {Neural Networks in Business Forecasting},
doi = {10.4018/978-1-59140-176-6.ch012}
}

@article{35,
  title={Long term time series prediction with multi-input multi-output local learning},
  author={Bontempi, Gianluca},
  journal={Proc. 2nd ESTSP},
  pages={145--154},
  year={2008}
}

@article{36,
  title={Conditionally dependent strategies for multiple-step-ahead prediction in local learning},
  author={Bontempi, Gianluca and Taieb, Souhaib Ben},
  journal={International journal of forecasting},
  volume={27},
  number={3},
  pages={689--699},
  year={2011},
  publisher={Elsevier}
}

@inproceedings{37,
  title={Long-term prediction of time series by combining direct and mimo strategies},
  author={Taieb, Souhaib Ben and Bontempi, Gianluca and Sorjamaa, Antti and Lendasse, Amaury},
  booktitle={2009 International Joint Conference on Neural Networks},
  pages={3054--3061},
  year={2009},
  organization={IEEE}
}

@book{38,
title = "Forecasting: Principles and Practice",
author = "Hyndman, {Robin John} and George Athanasopoulos",
year = "2018",
language = "English",
publisher = "OTexts",
address = "Australia",
edition = "2nd",
}

@article{39,
 ISSN = {01605682, 14769360},
 URL = {http://www.jstor.org/stable/4101650},
 abstract = {This paper considers univariate online electricity demand forecasting for lead times from a half-hour-ahead to a day-ahead. A time series of demand recorded at half-hourly intervals contains more than one seasonal pattern. A within-day seasonal cycle is apparent from the similarity of the demand profile from one day to the next, and a within-week seasonal cycle is evident when one compares the demand on the corresponding day of adjacent weeks. There is strong appeal in using a forecasting method that is able to capture both seasonalities. The multiplicative seasonal ARIMA model has been adapted for this purpose. In this paper, we adapt the Holt-Winters exponential smoothing formulation so that it can accommodate two seasonalities. We correct for residual autocorrelation using a simple autoregressive model. The forecasts produced by the new double seasonal Holt-Winters method outperform those from traditional Holt-Winters and from a well-specified multiplicative double seasonal ARIMA model.},
 author = {J. W. Taylor},
 journal = {The Journal of the Operational Research Society},
 number = {8},
 pages = {799--805},
 publisher = {Palgrave Macmillan Journals},
 title = {Short-Term Electricity Demand Forecasting Using Double Seasonal Exponential Smoothing},
 urldate = {2022-04-24},
 volume = {54},
 year = {2003}
}

@article{40,
title = {Forecasting third-party mobile payments with implications for customer flow prediction},
journal = {International Journal of Forecasting},
volume = {36},
number = {3},
pages = {739-760},
year = {2020},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2019.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0169207019302365},
author = {Shaohui Ma and Robert Fildes},
keywords = {Analytics, Big data, Customer flow forecasting, Machine learning, Forecasting many time series, Multi-step-ahead forecasting strategy},
abstract = {Forecasting customer flow is key for retailers in making daily operational decisions, but small retailers often lack the resources to obtain such forecasts. Rather than forecasting stores’ total customer flows, this research utilizes emerging third-party mobile payment data to provide participating stores with a value-added service by forecasting their share of daily customer flows. These customer transactions using mobile payments can then be utilized further to derive retailers’ total customer flows indirectly, thereby overcoming the constraints that small retailers face. We propose a third-party mobile-payment-platform centered daily mobile payments forecasting solution based on an extension of the newly-developed Gradient Boosting Regression Tree (GBRT) method which can generate multi-step forecasts for many stores concurrently. Using empirical forecasting experiments with thousands of time series, we show that GBRT, together with a strategy for multi-period-ahead forecasting, provides more accurate forecasts than established benchmarks. Pooling data from the platform across stores leads to benefits relative to analyzing the data individually, thus demonstrating the value of this machine learning application.}
}

@InProceedings{41,
author="Cortez, Paulo
and Matos, Lu{\'i}s Miguel
and Pereira, Pedro Jos{\'e}
and Santos, Nuno
and Duque, Duarte",
editor="Gra{\~{n}}a, Manuel
and L{\'o}pez-Guede, Jos{\'e} Manuel
and Etxaniz, Oier
and Herrero, {\'A}lvaro
and Quinti{\'a}n, H{\'e}ctor
and Corchado, Emilio",
title="Forecasting Store Foot Traffic Using Facial Recognition, Time Series and Support Vector Machines",
booktitle="International Joint Conference SOCO'16-CISIS'16-ICEUTE'16",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="267--276",
abstract="In this paper, we explore data collected in a pilot project that used a digital camera and facial recognition to detect foot traffic to a sports store. Using a time series approach, we model daily incoming store traffic under three classes (all faces, female, male) and compare six forecasting approaches, including Holt-Winters (HW), a Support Vector Machine (SVM) and a HW-SVM hybrid that includes other data features (e.g., weather conditions). Several experiments were held, under a robust rolling windows scheme that considers up to one week ahead predictions and two metrics (predictive error and estimated store benefit). Overall, competitive results were achieved by the SVM (all faces), HW (female) and HW-SVM (male) methods, which can potentially lead to valuable gains (e.g., enhancing store marketing or human resource management).",
isbn="978-3-319-47364-2"
}

@incollection{42,
  title={Time Series Forecasting in Retail Sales Using LSTM and Prophet},
  author={Junior, Clony and Gusm{\~a}o, Pedro and Moreira, Jos{\'e} and Tome, Ana Maria M},
  booktitle={Handbook of Research on Applied Data Science and Artificial Intelligence in Business and Industry},
  pages={241--262},
  year={2021},
  publisher={IGI Global}
}

@INPROCEEDINGS{43,
  author={Abrishami, Soheila and Kumar, Piyush},
  booktitle={2018 IEEE International Conference on Big Data (Big Data)}, 
  title={Using Real-World Store Data for Foot Traffic Forecasting}, 
  year={2018},
  volume={},
  number={},
  pages={1885-1890},
  doi={10.1109/BigData.2018.8622551}}
  
 @article{44,
author = {Christ, Maximilian and Kempa-Liehr, Andreas and Feindt, Michael},
year = {2016},
month = {10},
pages = {},
title = {Distributed and parallel time series feature extraction for industrial big data applications}
}

@article{45,
  author    = {Abien Fred Agarap},
  title     = {Deep Learning using Rectified Linear Units (ReLU)},
  journal   = {CoRR},
  volume    = {abs/1803.08375},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.08375},
  eprinttype = {arXiv},
  eprint    = {1803.08375},
  timestamp = {Mon, 13 Aug 2018 16:47:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-08375.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

